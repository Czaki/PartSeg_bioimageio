{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to improve data labeling using deep learning on PartSeg output\n",
    "\n",
    "In this tutorial we discuss how to train a new deep learning model based on (semi)automatic segmentation methods' output.\n",
    "\n",
    "PartSeg provides several (semi)automatic segmentation methods (see Fig....). They can be used to do segmentation on an input dataset but to achieve best results require human supervision. For example \"tu method name\" requires providing values for some parameters like \"tu parametry\". Also a single method may not be appropriate for the whole dataset and the human user needs to decide on combination of methods and division of the input data.\n",
    "In this tutorial we present how to use the output from previous segmentations done with PartSeg to train a deep learning model. Such model can then be added to PartSeg and used in a fully automated manner, i.e., to perform automatic segmentation of similar datasets without human supervision.\n",
    "\n",
    "![](images/marked_methods.png)\n",
    "\n",
    "The main adventage of using (semi)automatic method output is that preparing train and test sets is much faster and cheaper. On the other hand, automatic deep learning model offers usability improvement.\n",
    "\n",
    "<!-- There are multiple scenarios when having a working deep learning model could help:\n",
    "\n",
    "1) Used (semi)automatic method requires using a given probe or marking objects that are not required in the experiment. Then Segmentation could be done on specially prepared data, but a model train only using the subset of channels. Because some methods have a limited number of channels, it may allow marking and investigating more objects important from the point of scientific question. For example, confocal microscopes allow using only four channels.\n",
    "\n",
    "2) Sometimes, an available method requires some expensive (in the context of time) preprocessing steps like deconvolution.\n",
    "\n",
    "3) Collecting data with a low noise ratio may require access to limited and expensive infrastructure. However, collecting only data needed for the model training may be much more straightforward than collecting all experiment data. Then, the preprocessing phase could add artificial noise before starting the train. -->\n",
    "\n",
    "The data sets for this tutorial are available on [zenodo](https://zenodo.org/record/7335430). When training machine learning models it is important to split data into two parts *Train* and *Test*. The size of *Train* should be 80-90% od data, and *Test* should contain 10-20% of the data. Such approach allows to counteract model overfiting, i.e., obtaining models adapted too much to the particular input data set, but not working in general. When training algorithms on own data set it is important to also split them in such a way. It is also important that the data is cleaned, that is it does not contain errors nor omissions. The sample data we provide with this tutorial was aready cleaned and split. It is organized into 3 folders:\n",
    "\n",
    "* *Train* - data for train\n",
    "* *Test* - data for test\n",
    "* *Problematic* - data with a high number of artifacts. Could be used for manual verification the model.\n",
    "\n",
    "Each folder contains PartSeg projects with contains images and segmentations. The raw data on which this dataset is build is also available on [zenodo](https://zenodo.org/record/7335168).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google colab\n",
    "\n",
    "Run following code only if execute notebook on google colab to setup environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, make sure that you are using a GPU. For this, go to:\n",
    "# Runtime->Change runtime type and select Hardware accelarator->GPU\n",
    "# When you then run this cell you should see a gpu status overview\n",
    "# (if something went wrong you will see an error message)\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install conda in your google drive session\n",
    "!pip install -q condacolab\n",
    "import condacolab\n",
    "condacolab.install()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update mamba\n",
    "!mamba update mamba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install required libraries\n",
    "!mamba install -c pytorch -c conda-forge python-elf dask bioimageio.core tifffile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --no-deps kornia\n",
    "!pip install --no-deps PartSeg\n",
    "!pip install --no-deps git+https://github.com/constantinpape/torch-em\n",
    "!pip install --no-deps git+https://github.com/czaki/PartSeg_bioimageio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mount your google drive to permanently save data\n",
    "from google.colab import drive\n",
    "drive.mount(\"/content/gdrive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "data_path = os.path.join(\"data\", \"neu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To download data from zenodo please execute following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import zipfile\n",
    "import tempfile\n",
    "from urllib import request\n",
    "\n",
    "def download_and_unpack_data():\n",
    "    if os.path.exists(data_path) and os.listdir(data_path):\n",
    "        return\n",
    "    with tempfile.TemporaryDirectory() as temp_dir:\n",
    "        print(temp_dir)\n",
    "        zip_path = os.path.join(temp_dir, \"ecoli_neu_PartSeg_projects.zip\")\n",
    "        request.urlretrieve(\"https://zenodo.org/record/7335430/files/ecoli_neu_PartSeg_projects.zip?download=1\", zip_path)\n",
    "        os.makedirs(data_path)\n",
    "        with zipfile.ZipFile(zip_path) as zip_ref:\n",
    "            zip_ref.extractall(data_path)\n",
    "\n",
    "download_and_unpack_data()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It this tutorial we use [torch_em](https://github.com/constantinpape/torch-em) as a wrapper around [pytorch](https://pytorch.org/). Please read its installations [instruction](https://github.com/constantinpape/torch-em#installation). To keep readability of this document part of code will be in `train_util.py` file next to this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch_em.transform\n",
    "from torch_em.model import UNet2d\n",
    "\n",
    "from train_util import get_partseg_loader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Network parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NETWORK_DEPTH=4\n",
    "NETWORK_INITIAL_FEATURES=32\n",
    "PATCH_SIZE=256\n",
    "BATCH_SIZE=8\n",
    "ITERATIONS=500\n",
    "SAVE_ROOT=\"./checkpoint\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PartSeg allows that ROI Extraction (Data labeling) algorith could return multiple labeling\n",
    "of data for better visualization. The required one is \"ROI\" in which each component is marked with separate number.\n",
    "\n",
    "In the [Trapalyzer](https://github.com/Czaki/Trapalyzer) that is used for generate labeling in this project following additional labeling are provided:\n",
    "\n",
    "* PMN neu: polymorphonuclear, unstimulated neutrophils\n",
    "* RND neu: cells with decondensed chromatin and rounded nucleus\n",
    "* RUP neu: neutrophils with ruptured nuclear envelope\n",
    "* PER neu: neutrophils with a permeabilized plasma membrane\n",
    "* Bacteria: e-coli bacteria\n",
    "* NET: neutrophile net\n",
    "* Unknown intra: not classified objects for intracellurar probe\n",
    "* Unknown extra: not classified objects for extracellular probe\n",
    "* Labeling: Each class of objects are represented by a single number\n",
    "\n",
    "In this tutorial we would like to create model that will predict to which class belong a given object. Because of this we will use \"Labeling\" as input to model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME_OF_ROI_LABELING=\"Labeling\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because first four class that we want to diverse are different phases of same cell type we decide to use one hot encoding.\n",
    "Bellow is function that transform \"Labeling\" array into 7 channel image. (*TODO dodać informacje o wykrywaniu krawędzi jeśli zadziała*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trapalyzer_label_transform(labels):\n",
    "    one_hot = np.zeros((7,) + labels.shape, dtype=\"float32\")\n",
    "    for i in range(1, 6):\n",
    "        one_hot[i][labels == i] = 1\n",
    "    one_hot[6][labels == 8] = 1  # NET\n",
    "    one_hot[0][(labels == 0) | (labels > 8)] = 1  # Background\n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_and_val(data_dir, patch_size, batch_size):\n",
    "    train_loader = get_partseg_loader(\n",
    "        data_dir,\n",
    "        patch_shape=(patch_size, patch_size),\n",
    "        batch_size=batch_size,\n",
    "        split=\"train\",\n",
    "        label_transform=trapalyzer_label_transform,\n",
    "        label_name=NAME_OF_ROI_LABELING,\n",
    "    )\n",
    "    val_loader = get_partseg_loader(\n",
    "        data_dir,\n",
    "        patch_shape=(patch_size, patch_size),\n",
    "        batch_size=batch_size,\n",
    "        split=\"test\",\n",
    "        label_name=NAME_OF_ROI_LABELING,\n",
    "        label_transform=trapalyzer_label_transform,\n",
    "    )\n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNet2d(\n",
    "    in_channels=3,\n",
    "    out_channels=7,\n",
    "    depth=NETWORK_DEPTH,\n",
    "    initial_features=NETWORK_INITIAL_FEATURES,\n",
    ")\n",
    "train_loader, val_loader = get_train_and_val(data_path, PATCH_SIZE, BATCH_SIZE)\n",
    "\n",
    "MODEL_NAME = \"neutrofile_model\"\n",
    "\n",
    "trainer = torch_em.default_segmentation_trainer(\n",
    "    name=MODEL_NAME,\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    learning_rate=1e-4,\n",
    "    device=torch.device(\"cuda\"),\n",
    "    save_root=SAVE_ROOT,\n",
    ")\n",
    "trainer.fit(iterations=ITERATIONS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model exporting\n",
    "\n",
    "Used framework require additional export steep to save model in bioimageio format. The export steep need to be done on same machine as train because of hardcoded path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_em.util.modelzoo import export_bioimageio_model\n",
    "\n",
    "DOCUMENTATION = \"Text with model documentation\"\n",
    "\n",
    "export_bioimageio_model(\n",
    "    os.path.join(SAVE_ROOT, MODEL_NAME),\n",
    "    os.path.join(\"exports\", MODEL_NAME),\n",
    "    name=MODEL_NAME,\n",
    "    description=\"neutrofile model\",\n",
    "    authors=[{\"name\": \"Grzegorz Bokota\"}],\n",
    "    license=\"MIT\",\n",
    "    documentation=DOCUMENTATION,\n",
    "    git_repo=\"https://github.com/Czaki/entropy-calculation/\",\n",
    "    cite=\"\",\n",
    "    maintainers=[{\"github_user\": \"czaki\"}],\n",
    "    tags=[\n",
    "        \"2d\",\n",
    "        \"unet\",\n",
    "        \"fluorescence-light-microscopy\",\n",
    "        \"cells\",\n",
    "        \"pytorch\",\n",
    "        \"semantic-segmentation\",\n",
    "        \"ilastik\",\n",
    "    ],\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
